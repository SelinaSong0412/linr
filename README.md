
<!-- README.md is generated from README.Rmd. Please edit that file -->

# linr

<!-- badges: start -->

<!-- badges: end -->

“linr” is used to fit a linear model. In this function, linear
regression can be done by three matrix decomposition methods, which are
the QR decomposition, Cholesky decomposition and the singular value
decomposition (SVD). The defalt fitting method used is the Cholesky
decomposition method. All three decomposition methods can fit linear
model with high efficiency.

## Installation

You can install the development version of linr from
[GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("SelinaSong0412/linr")
#> Skipping install of 'linr' from a github remote, the SHA1 (d644867c) has not changed since last install.
#>   Use `force = TRUE` to force installation
```

## Example

The following are basic examples which shows you how to fit a linear
model with ‘linr’ You can look at the examples and practice with the
whole usage of ‘linr’.

First, library ‘linr’
function.

``` r
library(linr)
```

### Using linr with defined vector Y (observations) and vector/matrix X (predictions). (Without dataset)

**Conducting simple linear regression with ‘linr’**

You can fit your model like this:

``` r
y = rnorm(300)
x = rnorm(300)
model.linr <- linr(y ~ x)
```

Then you could check many items generated by ‘linr’ as follows:

``` r
model.linr$Call                      # This is how your model looks like
#> linr(formula = y ~ x)
data.frame(model.linr$coefficients,  # Coefficient estimators
           model.linr$std.error,     # Standard error of estimators
           model.linr$T_statistic,   # T statistics of estimators
           model.linr$p_value.T)     # p value for T test 
#>             model.linr.coefficients model.linr.std.error model.linr.T_statistic
#> (Intercept)              0.13985662           0.05800974              2.4109163
#> x                        0.04586778           0.05512748              0.8320312
#>             model.linr.p_value.T
#> (Intercept)           0.01651757
#> x                     0.40605756

# Other items also can be checked by 
data.frame(model.linr$MSE,           # Mean square error 
           model.linr$R.square,      # R^2 
           model.linr$Adj.R.square,  # Adjusted R^2 
           model.linr$F_statistic,   # F statistics of estimators
           model.linr$p_value.F)     # p value for F test 
#>   model.linr.MSE model.linr.R.square model.linr.Adj.R.square
#> 1       1.008318          0.00231769            -0.001030238
#>   model.linr.F_statistic model.linr.p_value.F
#> 1               0.692276            0.4060576

# You could also look at the fitted values and residuals like this:
head(model.linr$fitted.values)       # Look at the first 6 fitted values
#>         1         2         3         4         5         6 
#> 0.1525967 0.1430193 0.2210467 0.1487190 0.1531043 0.1954571
head(model.linr$residuals)           # Look at the first 6 residuals
#>          1          2          3          4          5          6 
#> -0.7989436  0.5114387 -2.0251929  1.0667278 -1.1805524 -0.6623024
```

**Conducting multiple linear regression with ‘linr’**

You can fit your multiple regression model with 3 options of matrix
decompositiom methods:

(1). QR decompositiom methods (2). SVD decompositiom methods (3).
Cholesky decompositiom methods (Default)

Here we first look at the default method:

``` r
Y = rnorm(300)
X = matrix(rnorm(6000), nrow = 300, ncol = 20)
model.linr.mul <- linr(Y ~ X, method = "cholesky")
# model.linr.mul <- linr(Y ~ X)                  # This do the same thing

# You can feel free to try the following alternative methods by yourself:
# model.linr.mul <- linr(Y ~ X, method = "qr")   # Using QR decomposition
# model.linr.mul <- linr(Y ~ X, method = "svd")  # Using SVD decomposition
```

Then you could check many items generated by ‘linr’ as
follows:

``` r
model.linr.mul$Call                      # This is how your model looks like
#> linr(formula = Y ~ X, method = "cholesky")
data.frame(model.linr.mul$coefficients,  # Coefficient estimators
           model.linr.mul$std.error,     # Standard error of estimators
           model.linr.mul$T_statistic,   # T statistics of estimators
           model.linr.mul$p_value.T)     # p value for T test 
#>             model.linr.mul.coefficients model.linr.mul.std.error
#> (Intercept)                -0.074264574               0.06040740
#> X1                         -0.053145406               0.05921851
#> X2                          0.012958519               0.05609091
#> X3                          0.040848807               0.06669230
#> X4                          0.008971084               0.06076053
#> X5                          0.049830034               0.06189882
#> X6                         -0.040990797               0.05926168
#> X7                         -0.016816957               0.06020601
#> X8                          0.079338241               0.05958332
#> X9                         -0.017362241               0.05909733
#> X10                        -0.026237534               0.06138220
#> X11                         0.042497523               0.05921952
#> X12                        -0.041619692               0.05798223
#> X13                         0.084687123               0.05880522
#> X14                        -0.003272730               0.05595713
#> X15                         0.091348467               0.06130025
#> X16                         0.016348209               0.05706257
#> X17                         0.034613962               0.06091525
#> X18                         0.013286902               0.06298354
#> X19                        -0.154795149               0.05756523
#> X20                         0.061953632               0.05887174
#>             model.linr.mul.T_statistic model.linr.mul.p_value.T
#> (Intercept)                -1.22939535               0.21995939
#> X1                         -0.89744587               0.37025481
#> X2                          0.23102708               0.81746309
#> X3                          0.61249656               0.54070815
#> X4                          0.14764658               0.88272834
#> X5                          0.80502392               0.42149137
#> X6                         -0.69169142               0.48970632
#> X7                         -0.27932357               0.78020347
#> X8                          1.33155123               0.18409457
#> X9                         -0.29379063               0.76913632
#> X10                        -0.42744533               0.66938470
#> X11                         0.71762688               0.47358780
#> X12                        -0.71780083               0.47348069
#> X13                         1.44012946               0.15095193
#> X14                        -0.05848638               0.95340308
#> X15                         1.49018101               0.13730627
#> X16                         0.28649618               0.77471075
#> X17                         0.56823148               0.57033484
#> X18                         0.21095834               0.83307378
#> X19                        -2.68903919               0.00759714
#> X20                         1.05234926               0.29354991

# Other items also can be checked by 
head(data.frame(model.linr.mul$MSE,           # Mean square error 
           model.linr.mul$R.square,      # R^2 
           model.linr.mul$Adj.R.square,  # Adjusted R^2 
           model.linr.mul$F_statistic,   # F statistics of estimators
           model.linr.mul$p_value.F))    # p value for F test 
#>   model.linr.mul.MSE model.linr.mul.R.square model.linr.mul.Adj.R.square
#> 1           1.007003              0.06595088                -0.001006049
#>   model.linr.mul.F_statistic model.linr.mul.p_value.F
#> 1                  0.9849747                0.4805304

# You could also look at the fitted values and residuals like this:
head(model.linr.mul$fitted.values)       # Look at the first 6 fitted values
#>          1          2          3          4          5          6 
#> -0.5121500  0.2904791 -0.2491057  0.1380434  0.1117976 -0.1176255
head(model.linr.mul$residuals)           # Look at the first 6 residuals
#>          1          2          3          4          5          6 
#>  0.2723953  1.8013609  0.1363735  0.2401183 -0.6491691 -1.0214279
```

## Using linr with regression formula and your own dataset

**Conducting simple linear regression with ‘linr’ on your data**

``` r
# Here we use the R build-in dataset cars as an example:
# fit a linear model with dist as outcome and speed as predictor
model.linr.cars <- linr(dist ~ speed, data = cars) 
```

Then you could check items generated by ‘linr’ as
follows:

``` r
model.linr.cars$Call                      # This is how your model looks like
#> linr(formula = dist ~ speed, data = cars)
data.frame(model.linr.cars$coefficients,  # Coefficient estimators
           model.linr.cars$std.error,     # Standard error of estimators
           model.linr.cars$T_statistic,   # T statistics of estimators
           model.linr.cars$p_value.T)     # p value for T test 
#>             model.linr.cars.coefficients model.linr.cars.std.error
#> (Intercept)                   -17.579095                 6.7584402
#> speed                           3.932409                 0.4155128
#>             model.linr.cars.T_statistic model.linr.cars.p_value.T
#> (Intercept)                   -2.601058              1.231882e-02
#> speed                          9.463990              1.489836e-12

# Other items also can be checked by 
data.frame(model.linr.cars$MSE,           # Mean square error 
           model.linr.cars$R.square,      # R^2 
           model.linr.cars$Adj.R.square,  # Adjusted R^2 
           model.linr.cars$F_statistic,   # F statistics of estimators
           model.linr.cars$p_value.F)     # p value for F test 
#>   model.linr.cars.MSE model.linr.cars.R.square model.linr.cars.Adj.R.square
#> 1            236.5317                0.6510794                    0.6438102
#>   model.linr.cars.F_statistic model.linr.cars.p_value.F
#> 1                    89.56711              1.489836e-12

# You could also look at the fitted values and residuals like this:
head(model.linr.cars$fitted.values)       # Look at the first 6 fitted values
#>         1         2         3         4         5         6 
#> -1.849460 -1.849460  9.947766  9.947766 13.880175 17.812584
head(model.linr.cars$residuals)           # Look at the first 6 residuals
#>         1         2         3         4         5         6 
#>  3.849460 11.849460 -5.947766 12.052234  2.119825 -7.812584
```

**Conducting multiple linear regression with ‘linr’ on your data**

``` r
# Here we use the R build-in dataset mtcars as an example:
# fit a linear model with disp as outcome and mpg, wt, carb as predictors
model.linr.mtcars <- linr(disp ~ mpg + wt + carb, data = mtcars)
```

Then you could check items generated by ‘linr’ as
follows:

``` r
model.linr.mtcars$Call                      # This is how your model looks like
#> linr(formula = disp ~ mpg + wt + carb, data = mtcars)
data.frame(model.linr.mtcars$coefficients,  # Coefficient estimators
           model.linr.mtcars$std.error,     # Standard error of estimators
           model.linr.mtcars$T_statistic,   # T statistics of estimators
           model.linr.mtcars$p_value.T)     # p value for T test 
#>             model.linr.mtcars.coefficients model.linr.mtcars.std.error
#> (Intercept)                     143.670967                  142.740385
#> mpg                              -7.304425                    3.669182
#> wt                               76.663396                   20.865454
#> carb                             -4.566736                    7.529811
#>             model.linr.mtcars.T_statistic model.linr.mtcars.p_value.T
#> (Intercept)                     1.0065194                0.3227853099
#> mpg                            -1.9907502                0.0563492712
#> wt                              3.6741782                0.0009992845
#> carb                           -0.6064875                0.5490772554

# Other items also can be checked by 
data.frame(model.linr.mtcars$MSE,           # Mean square error 
           model.linr.mtcars$R.square,      # R^2 
           model.linr.mtcars$Adj.R.square,  # Adjusted R^2 
           model.linr.mtcars$F_statistic,   # F statistics of estimators
           model.linr.mtcars$p_value.F)     # p value for F test 
#>   model.linr.mtcars.MSE model.linr.mtcars.R.square
#> 1              3146.542                  0.8149811
#>   model.linr.mtcars.Adj.R.square model.linr.mtcars.F_statistic
#> 1                      0.7951576                      41.11196
#>   model.linr.mtcars.p_value.F
#> 1                2.171366e-10

# You could also look at the fitted values and residuals like this:
head(model.linr.mtcars$fitted.values)       # Look at the first 6 fitted values
#>         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive 
#>          172.8692          192.4184          150.4224          229.2624 
#> Hornet Sportabout           Valiant 
#>          261.6668          272.1495
head(model.linr.mtcars$residuals)           # Look at the first 6 residuals
#>         Mazda RX4     Mazda RX4 Wag        Datsun 710    Hornet 4 Drive 
#>         -12.86920         -32.41837         -42.42243          28.73764 
#> Hornet Sportabout           Valiant 
#>          98.33316         -47.14949
```
