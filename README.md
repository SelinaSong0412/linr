
<!-- README.md is generated from README.Rmd. Please edit that file -->

# linr

<!-- badges: start -->

[![R-CMD-check](https://github.com/SelinaSong0412/linr/workflows/R-CMD-check/badge.svg)](https://github.com/SelinaSong0412/linr/actions)
[![codecov](https://codecov.io/gh/SelinaSong0412/linr/branch/main/graph/badge.svg?token=K6NRF4WUNZ)](https://codecov.io/gh/SelinaSong0412/linr)
<!-- badges: end -->

“linr” is used to fit a linear model. In this function, linear
regression can be done by three matrix decomposition methods, which are
the QR decomposition, Cholesky decomposition and the singular value
decomposition (SVD). The defalt fitting method used is the Cholesky
decomposition method. All three decomposition methods can fit linear
model with high efficiency.

## Installation

You can install the development version of linr from
[GitHub](https://github.com/) with:

``` r
# install.packages("devtools")
devtools::install_github("SelinaSong0412/linr")
#> Downloading GitHub repo SelinaSong0412/linr@HEAD
#>      checking for file ‘/private/var/folders/_r/4mdt4w5x1wz3_hchxcgymgf40000gn/T/RtmpEVPtuE/remotes1487c10031018/SelinaSong0412-linr-5854a8e/DESCRIPTION’ ...  ✓  checking for file ‘/private/var/folders/_r/4mdt4w5x1wz3_hchxcgymgf40000gn/T/RtmpEVPtuE/remotes1487c10031018/SelinaSong0412-linr-5854a8e/DESCRIPTION’
#>      preparing ‘linr’  ─  preparing ‘linr’:
#>    checking DESCRIPTION meta-information ...  ✓  checking DESCRIPTION meta-information
#>   ─  checking for LF line-endings in source and make files and shell scripts
#>   ─  checking for empty or unneeded directories
#>   ─  building ‘linr_0.1.0.tar.gz’
#>      
#> 
```

## Example

The following are basic examples which shows you how to fit a linear
model with ‘linr’ You can look at the examples and practice with the
whole usage of ‘linr’.

First, library ‘linr’
function.

``` r
library(linr)
```

### Using linr with defined vector Y (observations) and vector/matrix X (predictions). (Without dataset)

**Conducting simple linear regression with ‘linr’**

You can fit your model like this:

``` r
y = rnorm(300)
x = rnorm(300)
model.linr <- linr(y ~ x)
```

Then you could check many items generated by ‘linr’ as follows:

``` r
model.linr$Call                      # This is how your model looks like
#> linr(formula = y ~ x)
data.frame(model.linr$coefficients,  # Coefficient estimators
           model.linr$std.error,     # Standard error of estimators
           model.linr$T_statistic,   # T statistics of estimators
           model.linr$p_value.T)     # p value for T test 
#>             model.linr.coefficients model.linr.std.error model.linr.T_statistic
#> (Intercept)            6.967282e-02           0.05812594            1.198652813
#> x                     -8.761394e-05           0.05873288           -0.001491736
#>             model.linr.p_value.T
#> (Intercept)            0.2316153
#> x                      0.9988108

# Other items also can be checked by 
data.frame(model.linr$MSE,           # Mean square error 
           model.linr$R.square,      # R^2 
           model.linr$Adj.R.square,  # Adjusted R^2 
           model.linr$F_statistic,   # F statistics of estimators
           model.linr$p_value.F)     # p value for F test 
#>   model.linr.MSE model.linr.R.square model.linr.Adj.R.square
#> 1       1.011384        7.467368e-09            -0.003355697
#>   model.linr.F_statistic model.linr.p_value.F
#> 1           2.225276e-06            0.9988108

# You could also look at the fitted values and residuals like this:
# head(model.linr$fitted.values)       # Look at the first 6 fitted values
# head(model.linr$residuals)           # Look at the first 6 residuals
```

**Conducting multiple linear regression with ‘linr’**

You can fit your multiple regression model with 3 options of matrix
decompositiom methods:

(1). QR decompositiom methods (2). SVD decompositiom methods (3).
Cholesky decompositiom methods (Default)

Here we first look at the default method:

``` r
Y = rnorm(300)
X = matrix(rnorm(6000), nrow = 300, ncol = 20)
model.linr.mul <- linr(Y ~ X, method = "cholesky")
# model.linr.mul <- linr(Y ~ X)                  # This do the same thing

# You can feel free to try the following alternative methods by yourself:
# model.linr.mul <- linr(Y ~ X, method = "qr")   # Using QR decomposition
# model.linr.mul <- linr(Y ~ X, method = "svd")  # Using SVD decomposition
```

Then you could check many items generated by ‘linr’ as
follows:

``` r
model.linr.mul$Call                      # This is how your model looks like
#> linr(formula = Y ~ X, method = "cholesky")
data.frame(model.linr.mul$coefficients,  # Coefficient estimators
           model.linr.mul$std.error,     # Standard error of estimators
           model.linr.mul$T_statistic,   # T statistics of estimators
           model.linr.mul$p_value.T)     # p value for T test 
#>             model.linr.mul.coefficients model.linr.mul.std.error
#> (Intercept)                -0.069316084               0.06342209
#> X1                         -0.033469478               0.06145244
#> X2                         -0.016685846               0.06262762
#> X3                         -0.011759906               0.06602438
#> X4                         -0.023449794               0.06356825
#> X5                          0.001626069               0.06816305
#> X6                         -0.059660515               0.06549996
#> X7                          0.113949987               0.06500186
#> X8                          0.050928935               0.06569159
#> X9                          0.007733220               0.06546342
#> X10                        -0.062174065               0.06205717
#> X11                        -0.016541237               0.06292192
#> X12                        -0.008591924               0.06853914
#> X13                         0.096573455               0.06975633
#> X14                         0.063385431               0.07010726
#> X15                        -0.028888862               0.06661836
#> X16                         0.126467453               0.07120476
#> X17                         0.104587020               0.06168259
#> X18                         0.027777026               0.06490101
#> X19                         0.016347450               0.06648398
#> X20                        -0.015331052               0.06674606
#>             model.linr.mul.T_statistic model.linr.mul.p_value.T
#> (Intercept)                -1.09293275               0.27536611
#> X1                         -0.54464031               0.58643597
#> X2                         -0.26642949               0.79010526
#> X3                         -0.17811460               0.85876224
#> X4                         -0.36889160               0.71248830
#> X5                          0.02385558               0.98098486
#> X6                         -0.91084807               0.36316170
#> X7                          1.75302657               0.08069572
#> X8                          0.77527323               0.43883482
#> X9                          0.11813039               0.90604934
#> X10                        -1.00188359               0.31726795
#> X11                        -0.26288513               0.79283314
#> X12                        -0.12535792               0.90033048
#> X13                         1.38444010               0.16732999
#> X14                         0.90412084               0.36671133
#> X15                        -0.43364712               0.66487982
#> X16                         1.77610958               0.07680467
#> X17                         1.69556795               0.09108331
#> X18                         0.42799068               0.66898809
#> X19                         0.24588556               0.80595161
#> X20                        -0.22969224               0.81849923

# Other items also can be checked by 
head(data.frame(model.linr.mul$MSE,           # Mean square error 
           model.linr.mul$R.square,      # R^2 
           model.linr.mul$Adj.R.square,  # Adjusted R^2 
           model.linr.mul$F_statistic,   # F statistics of estimators
           model.linr.mul$p_value.F))    # p value for F test 
#>   model.linr.mul.MSE model.linr.mul.R.square model.linr.mul.Adj.R.square
#> 1           1.137619              0.05094612                 -0.01708642
#>   model.linr.mul.F_statistic model.linr.mul.p_value.F
#> 1                  0.7488493                0.7732871

# You could also look at the fitted values and residuals like this:
# head(model.linr.mul$fitted.values)       # Look at the first 6 fitted values
# head(model.linr.mul$residuals)           # Look at the first 6 residuals
```

## Using linr with regression formula and your own dataset

**Conducting simple linear regression with ‘linr’ on your data**

``` r
# Here we use the R build-in dataset cars as an example:
# fit a linear model with dist as outcome and speed as predictor
model.linr.cars <- linr(dist ~ speed, data = cars) 
```

Then you could check items generated by ‘linr’ as
follows:

``` r
model.linr.cars$Call                      # This is how your model looks like
#> linr(formula = dist ~ speed, data = cars)
data.frame(model.linr.cars$coefficients,  # Coefficient estimators
           model.linr.cars$std.error,     # Standard error of estimators
           model.linr.cars$T_statistic,   # T statistics of estimators
           model.linr.cars$p_value.T)     # p value for T test 
#>             model.linr.cars.coefficients model.linr.cars.std.error
#> (Intercept)                   -17.579095                 6.7584402
#> speed                           3.932409                 0.4155128
#>             model.linr.cars.T_statistic model.linr.cars.p_value.T
#> (Intercept)                   -2.601058              1.231882e-02
#> speed                          9.463990              1.489836e-12

# Other items also can be checked by 
data.frame(model.linr.cars$MSE,           # Mean square error 
           model.linr.cars$R.square,      # R^2 
           model.linr.cars$Adj.R.square,  # Adjusted R^2 
           model.linr.cars$F_statistic,   # F statistics of estimators
           model.linr.cars$p_value.F)     # p value for F test 
#>   model.linr.cars.MSE model.linr.cars.R.square model.linr.cars.Adj.R.square
#> 1            236.5317                0.6510794                    0.6438102
#>   model.linr.cars.F_statistic model.linr.cars.p_value.F
#> 1                    89.56711              1.489836e-12

# You could also look at the fitted values and residuals like this:
# head(model.linr.cars$fitted.values)       # Look at the first 6 fitted values
# head(model.linr.cars$residuals)           # Look at the first 6 residuals
```

**Conducting multiple linear regression with ‘linr’ on your data**

``` r
# Here we use the R build-in dataset mtcars as an example:
# fit a linear model with disp as outcome and mpg, wt, carb as predictors
model.linr.mtcars <- linr(disp ~ mpg + wt + carb, data = mtcars)
```

Then you could check items generated by ‘linr’ as
follows:

``` r
model.linr.mtcars$Call                      # This is how your model looks like
#> linr(formula = disp ~ mpg + wt + carb, data = mtcars)
data.frame(model.linr.mtcars$coefficients,  # Coefficient estimators
           model.linr.mtcars$std.error,     # Standard error of estimators
           model.linr.mtcars$T_statistic,   # T statistics of estimators
           model.linr.mtcars$p_value.T)     # p value for T test 
#>             model.linr.mtcars.coefficients model.linr.mtcars.std.error
#> (Intercept)                     143.670967                  142.740385
#> mpg                              -7.304425                    3.669182
#> wt                               76.663396                   20.865454
#> carb                             -4.566736                    7.529811
#>             model.linr.mtcars.T_statistic model.linr.mtcars.p_value.T
#> (Intercept)                     1.0065194                0.3227853099
#> mpg                            -1.9907502                0.0563492712
#> wt                              3.6741782                0.0009992845
#> carb                           -0.6064875                0.5490772554

# Other items also can be checked by 
data.frame(model.linr.mtcars$MSE,           # Mean square error 
           model.linr.mtcars$R.square,      # R^2 
           model.linr.mtcars$Adj.R.square,  # Adjusted R^2 
           model.linr.mtcars$F_statistic,   # F statistics of estimators
           model.linr.mtcars$p_value.F)     # p value for F test 
#>   model.linr.mtcars.MSE model.linr.mtcars.R.square
#> 1              3146.542                  0.8149811
#>   model.linr.mtcars.Adj.R.square model.linr.mtcars.F_statistic
#> 1                      0.7951576                      41.11196
#>   model.linr.mtcars.p_value.F
#> 1                2.171366e-10

# You could also look at the fitted values and residuals like this:
# head(model.linr.mtcars$fitted.values)       # Look at the first 6 fitted values
# head(model.linr.mtcars$residuals)           # Look at the first 6 residuals
```
